setwd("File Path")

#Acessar Twitter Dev
consumer_key <- "consumer key"
consumer_secret <- "consumer password"
access_token <- "access token"
access_secret <- "access token password"

#Carregar uma lista de polaridades que encontrei em fóruns e blogs online
polaridades_palavras <- read_csv('polaridades_palavras.csv')
stopwordslist <- read_csv('stopwords.csv')
stopwordslist

#Tags polarizadas
twitter_tags <- "#bolsomito|#bolsonaro|#bolsolixo|#haddad|#elenao|#elesim|#elenao|#elesim"

# autorizar Twitter 
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret);
tweets <- searchTwitter(twitter_tag, lang = 'pt', resultType="mixed", n=5000);

tweetxt <- sapply(tweets, function(x) x$getText())
tibble(tweetxt)

removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
tweetxtUtf <-  readr::parse_character(tweetxt, locale = readr::locale('pt')) # sapply(tweetxt, function(x) iconv(x, "UTF-8"))
tweetxtUtf <- sapply(tweetxtUtf, function(x) stri_trans_tolower(x,'pt'))
tweetxtUtf <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ",  tweetxtUtf);
tweetxtUtf <- str_replace(tweetxtUtf,"RT @[a-z,A-Z]*: ","")
tweetxtUtf <- gsub("@\\w+", "", tweetxtUtf)
tweetxtUtf <- removeURL(tweetxtUtf)
tweetxtUtf <- str_replace_all(tweetxtUtf,"@[a-z,A-Z]*","")  
tweetxtUtf <- gsub("[^[:alnum:][:blank:]]", " ", tweetxtUtf)
tweetxtUtf <- gsub("[[:digit:]]", "", tweetxtUtf)
tweetxtUtfUnique <- tweetxtUtf %>% unique() 
tibble(tweetxtUtfUnique) 

#Criar um corpus 
library(tm)
tweetencoded <- sapply(tweetxtUtfUnique,enc2native)
df <- data.frame(text=tweetencoded)
head(df)

#Criar um id para o documento
df$doc_id <- row.names(df)
head(df)

tm_corpus <- Corpus(DataframeSource(df))
inspect(tm_corpus[1:4])

#criar o dtm usando palavras que tem mais de 3 letras e uma frequencia >19
dtm <- DocumentTermMatrix(tm_corpus, control=list(wordLengths=c(4, 20),language=locale('pt'), stopwords=stopwords('portuguese'),
                                                 bounds = list(global = c(30,500))))
dtm

#Ver os termos mais frequentes
findFreqTerms(dtm)

ttm_results <- t(as.matrix(dtm)) %*% as.matrix(dtm)
head(ttm_results)

#Instalar o pacote igraph
library(igraph)

#construir um grafico da matriz acima
g <- graph.adjacency(ttm_results, weighted=T, mode = 'undirected')
g <- simplify(g)

#setar o label e degree dos vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
E(g)$color <- ifelse(E(g)$weight > 15, "blue", "red")

set.seed(2000)
layout1 <- layout_on_sphere(g)
#plot(g, layout=layout1)
#tkplot(g, layout=layout.drl)
#plot(g, layout=layout.kamada.kawai)
plot(g, layout=layout.fruchterman.reingold)

#melhorar a apresentação de acordo com o peso dos termos
V(g)$label.cex <- 2.2 * V(g)$degree / max(V(g)$degree)+ .2
V(g)$label.color <- rgb(0, 0, .2, .8)
V(g)$frame.color <- NA
egam <- (log(E(g)$weight)+.4) / max(log(E(g)$weight)+.4)
E(g)$color <- rgb(.5, .5, 0, egam)
E(g)$width <- egam
plot(g, layout=layout1)

#Criar matrizes de termo e documentos

#No quanteda não precisa fazer um encode de todo o texto
library(quanteda)

#Qualificar os dados para futuramente fazer alguma coisa com isso
twdf <- tibble(tweet = tweetxtUtfUnique)
twdf$whois <- NA
twdf$whois[twdf$tweet %like% 'bolsonaro'] <- 'bolsonaro'
twdf$whois[twdf$tweet %like% 'haddad'] <- 'haddad'
twdf$whois[is.na(twdf$whois) ] <- 'semtag'
freq <- twdf %>% count(whois, sort = T) %>% select( whois,freq = n) 
freq

barplot(table(twdf$whois))

distMatrix <- as.matrix(dist(freq$freq))
plot(density(distMatrix))

#criando o dtm
dfq <- data.frame(id=row.names(twdf),
                  text=twdf$tweet, whois = factor(twdf$whois))

myCorpus <- corpus(twdf,  text_field = 'tweet', 
                   metacorpus = list(source = "tweets do bolsonaro e do haddad")) 
myCorpus

head(textstat_readability(myCorpus),2)


#observar os resultos
summary(myCorpus,6)

#acessar qualquer parte da matriz com a função texts
texts(myCorpus)[28:30]
summary(corpus_subset(myCorpus, whois == 'bolsonaro'),6)

#A função kwic (keywords in context) procura o texto e nos mostra uma forma visual da matrix
kwic(myCorpus,'virtual')

#com o quanteda tambem podemos tokenizar o texto, vamos pegar por exemplo nossos tweets originais
temptok <- tokens(tweetxtUtfUnique)
#note o objeto token
temptok[1:5]

remove(temptok)

#Usar a principal funçao do quanteda a dfm, que transforma o corpus em um 
#documento termo matriz e ao contrario da funçao tokens ela aplica varios funções de limpeza como
#retirar pontuação, converter para minusculo, para saber mais consulte a documentação
myDfm <- dfm(myCorpus, stem = F)
myDfm

topfeatures(myDfm,20)


stopwors2 <- c('the','r','é','c','?','!','of','rt','pra','pq', 'qd', 'mds')
myDfm <- dfm(myCorpus, groups='whois', remove = c(quanteda::stopwords("portuguese"),stopwors2,tm::stopwords('portuguese')), 
                 stem = F, remove_punct = TRUE)
                 
#Usar a opcão groups para agrupar pela minha classificação
myDfm

#Acessar os termos mais usados
topfeatures(myDfm, 20) 

#Wordcloud
set.seed(100)
textplot_wordcloud(myDfm, min.freq = 15, random.order = FALSE,
                   rot.per = .6, 
                   colors = RColorBrewer::brewer.pal(8,"Dark"))
                   
#Frequencia do texto 
allfeats <- textstat_frequency(myDfm)
allfeats$feature <- with(allfeats, reorder(feature, -frequency))

ggplot(head(allfeats,20), aes(x=feature, y=frequency, fill=frequency)) + geom_bar(stat="identity") +
xlab("Termos") + ylab("Frequência") + coord_flip() +
theme(axis.text=element_text(size=7))

#Quais sao os termos mais frequentes? 
col <- textstat_collocations(myCorpus , size = 2:4, min_count = 2)
head(col)

#col <- with(col, reorder(collocation, count))
ggplot(col[order(col$count, decreasing = T),][1:25,], 
       aes(x=reorder(collocation,count), y=factor(count), fill=factor(count))) + geom_bar(stat="identity") +
xlab("Expressões") + ylab("Frequência")  + coord_flip() +
theme(axis.text=element_text(size=7))

#Ver as palavras relacionadas do agrupamentos que criei
tstatkeyness <- textstat_keyness(myDfm, target = 'linkedin')
head(tstatkeyness,15)

#Plotar
textplot_keyness(tstatkeyness)

#dfm_sort(myDfm)[, 1:20]
textstat_simil(myDfm, c('microsoft','linkedin'))

##Plotar score wordfish(escala não supervisionada)
wfm <- textmodel_wordfish(myDfm)
textplot_scale1d(wfm)


#Plotar estimated word positions, Escala 1D
textplot_scale1d(wfm, margin = "features", 
                 highlighted = c("google", "microsoft", "bolsonaro", 
                                 "apple","linkedin"))
                                 
 #Plotar Xray
 textplot_xray(kwic(myCorpus[1:40], "haddad"), 
              kwic(myCorpus[1:40], "bolsonaro"),
              
#Calcular a diversidade complexidade dos textos
textstat_lexdiv(myDfm, "all",drop=T) %>% arrange(desc(U))

#Exibir colocations, fazendo um score de termos; pegar os tweets crus sem nenhum processo, apenas com encode
twraw <- readr::parse_character(tweetxt, locale = readr::locale('pt')) 
mytoken <- tokens(twraw, 
                  remove_numbers=T,remove_symbols=T, 
                  remove_twitter=T, remove_url=T)
head(mytoken)

mytoken <- tokens_remove(mytoken, stopwords('portuguese'))
head(textstat_collocations(mytoken,size = 5, min_count = 5))

#Criar um grafico das relações entre as tags
myrawCorpus <- corpus(twraw)
tweetdfm <- dfm(myrawCorpus, remove_punct = TRUE)
tagdfm <- dfm_select(tweetdfm, ('#*'))
toptag <- names(topfeatures(tagdfm, 50))
head(toptag)

tagfcm <- fcm(tagdfm)
head(tagfcm)

toptagfcm <- fcm_select(tagfcm, toptag)
textplot_network(toptagfcm, min_freq = 0.1, edge_alpha = 0.8, edge_size = 5)

#Criar um grafico das relações de usuários
userdfm <- dfm_select(tweetdfm, ('@*'))
topuser <- names(topfeatures(userdfm, 200))
userfcm <- fcm(userdfm)
userfcm <- fcm_select(userfcm, topuser)
textplot_network(userfcm, min_freq = 0.1, edge_color = 'blue', edge_alpha = 0.8, edge_size = 5)

#Criar um DTM com nossas polaridades como dicionario
positivas <- polaridades_pt %>% filter(sentimento == 'positivo') %>% select(word)
negativas <- polaridades_pt %>% filter(sentimento == 'negativo') %>% select(word)

dic <- dictionary(list(positivas=as.character(positivas$word), negativas=as.character(negativas$word)))
bySentimento <- dfm(myCorpus, dictionary = dic)
library(tidytext)
scorebygroup <- tidy(bySentimento %>% 
  dfm_group(groups='whois') )
scorebygroup

library(ggplot2)
library(scales)
scorebygroup %>%
  ggplot(aes(document, count)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~ term) +
  scale_y_continuous(labels = percent_format()) +
  ylab("Frequência por polaridade") +
  aes(color = term) + scale_color_manual(values = c("red", "green"))
  
  scorebygroup %>%
  ggplot(aes(term, count)) +
  geom_bar(stat = 'identity') +
  geom_smooth() +
  facet_wrap(~ document) +
  scale_y_continuous(labels = percent_format()) +
  ylab("Frequência por polaridade") +
  aes(fill= term,color = term) + scale_color_manual(values = c("red", "green"))
  
  #Procurar por frequencia
  bySentimento
  
  CreatePercentile <- function(x) {
    x$FreqTotal <- cumsum(x$frequency)
    TotalType <- sum(x$frequency)
    WordCoverage = NULL
    for (i in 1:10) {
        WordCoverage <- rbind(WordCoverage, c(max(x$rank[x$FreqTotal <= i/10 * 
            TotalType]), i/10))
    }
    Percentile <- c(WordCoverage[, 2])
    TextNum <- c(WordCoverage[, 1])
    WordCoverage <- data.frame(Percentile, TextNum)
    return(WordCoverage)
}
bolsonaroCorpus <- corpus_subset(myCorpus, whois=='bolsonaro')
haddadCorpus <- corpus_subset(myCorpus, whois=='haddad')
#teste <- dfm(myCorpus, dictionary = dic, groups = 'whois')
bolsofm <- dfm(bolsonaroCorpus, stem = F)
haddadfm <- dfm(haddadCorpus, stem = F)

bolsonarofreq <- textstat_frequency(bolsonarodfm)
haddadfreq <- textstat_frequency(haddaddfm)

bolsonarocover <-CreatePercentile(bolsonarofreq)
haddadcover <-CreatePercentile(haddadfreq)

faceg <- ggplot(facecover, aes(Percentile * 100, TextNum)) + geom_point() + 
    geom_line() + xlab("Cover") + ylab("# of Text ") + ggtitle("facebook")

googleg <- ggplot(googlecover, aes(Percentile * 100, TextNum)) + geom_point() + 
    geom_line() + xlab("Cover") + ylab("# of Text ") + ggtitle("google")

micg <- ggplot(miccover, aes(Percentile * 100, TextNum)) + geom_point() + 
    geom_line() + xlab("Cover") + ylab("# of Text ") + ggtitle("microsoft")
library(grid)
library(gridExtra)
grid.arrange(faceg, googleg, micg, ncol = 3)

Frequência relativa por empresa
gf <- myCorpus %>%
     dfm(remove = stopwords("portuguese"), remove_punct = TRUE) %>%
     dfm_weight(type = "relfreq")
# Calculando a frequencia relativa pelas empresas de tecnologia
freq_weight <- textstat_frequency(gf, n = 10, groups = "whois")

ggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +
     geom_point() +
     facet_wrap(~ group, scales = "free") +
     coord_flip() +
     scale_x_continuous(breaks = nrow(freq_weight):1,
                        labels = freq_weight$feature) +
     labs(x = NULL, y = "Relative frequency")
     
     Plotando um dendograma com quantide
#data(data_corpus_SOTU, package = "quantedaData")
dendoDfm <- dfm(myCorpus, 
               stem = TRUE, groups = 'whois', remove_punct = TRUE,
               remove = stopwords("portuguese"))
trimDfm <- dfm_trim(dendoDfm, min_count = 5, min_docfreq = 3)

distMat <- textstat_dist(dfm_weight(trimDfm, "relfreq"))

myCluster <- hclust(distMat)

myCluster$labels <- docnames(trimDfm)
# plot as a dendrogram
plot(myCluster, xlab = "", sub = "", main = "Distancia euclidiana na frequencia de tokens normalizada")

Procurando por similaridades
sim <- textstat_simil(myDfm, c("xbox"), method = "cosine", margin = "features")
lapply(as.list(sim), head, 20)

lista <- lapply(as.list(sim), head, 20)
dotchart(lista$xbox, xlab = "metodo cosine")

Trabalhando com modelos Topicos
library(topicmodels)
quantdfm <- dfm(myCorpus, 
                remove_punct = TRUE, remove_numbers = TRUE, tolower = T,  remove = stopwords("portuguese"))
quantdfm <- dfm_trim(quantdfm, min_count = 10, max_docfreq = 10, verbose = TRUE)


mylda <- LDA(convert(quantdfm, to = "topicmodels"), k = 20)
  #str(mylda)
  #mylda@documents

head(get_terms(mylda,6))

#Análise de Sentimentos com Tidy
#salvando o dataframe para não ter discrepancias nos comentários
#twdf %>% write_csv(path='parte2/twittersentimentaldata.csv')
twdf <- read_csv('d://Cursos/PreparacaoCarreiraCientista/R-Bigdata/Projeto1/parte2/twittersentimentaldata.csv')
twdf$id <- rownames(twdf)
tw <- twdf %>% mutate(document = id,word=tweet) %>% select(document,word,whois)
#note que a coluna document carrega a identificação de cada texto
str(tw)

tdm <- tw %>% unnest_tokens(word,word) 

#Removendo as stopwords
tdm <- tdm %>% anti_join(data.frame(word= stopwords('portuguese')))
tdm <- tdm %>% anti_join(data.frame(word= stopwors2))
head(tdm)

#tdm <- tdm  %>% group_by(document) %>% mutate(word_per_doc = n()) 
#tdm <- tdm  %>% group_by(whois) %>% mutate(word_per_whois = n()) 

#Plotando as primeiras impressões
library(tidyr)
sentJoin <- tdm %>%
              inner_join(polaridades_pt, by='word')

sentJoin %>%
  count(sentimento) %>%
  ggplot(aes(sentimento,n , fill = sentimento)) +
  geom_bar(stat = "identity", show.legend = FALSE)
  
  sentJoin %>%
  count(whois, index = document, sentimento) %>%
  spread(sentimento, n, fill = 0) %>%
  mutate(score = positivo - negativo) %>%
  ggplot(aes(index, score, fill = whois)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~whois, ncol = 2, scales = "free_x")
  
  scored <- sentJoin %>%
        count(whois,sentimento) %>%
        spread(sentimento, n, fill = 0) %>%
        mutate(score = positivo -negativo) %>%
        mutate(scoreperc = (positivo / (positivo + negativo)) * 100)
  ggplot(scored, aes(whois,scoreperc , fill = whois)) +
  geom_bar(stat = "identity", show.legend = T) 
  
  #Quais são nossos tops sentimentos?
word_counts <- sentJoin %>%
            count(word, sentimento, sort = TRUE) %>%
            ungroup()

word_counts %>%
  filter(n > 5) %>%
  mutate(n = ifelse(sentimento == "negativo", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentimento)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")
  
#E finalmente nossa wordcloud de sentimentos
library(reshape2)
library(wordcloud)
sentJoin %>%
count(word, sentimento, sort = TRUE) %>%
  acast(word ~ sentimento, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                  max.words = 60)
                  
#Sentimentos mais negativos
bottom8tw <- head(sentJoin %>%
            count(document, sentimento) %>%
            spread(sentimento, n, fill = 0) %>%
            mutate(score = positivo - negativo) %>%
            arrange(score),8)['document']


twdf %>% filter(id %in% as.vector(bottom8tw$document))

#Sentimentos mais Positivos
top8 <- head(sentJoin %>%
            count(document, sentimento) %>%
            spread(sentimento, n, fill = 0) %>%
            mutate(score = positivo - negativo) %>%
            arrange(desc(score)),8)['document']
twdf %>% filter(id %in% as.vector(top8$document))

#Analise de Sentimentos usando Naive Bayes
#devtool::install_github('mananshah99/sentR')
require(sentR)

sentimentToScore <-  sample_n(data.frame(text=twdf$tweet),100)
# 2. Aplicando o metodo de classificação Naive Bayes
out <- classify.naivebayes(sentimentToScore$text)
scoredDf <- cbind(sentimentToScore,out, stringsAsFactors=F)
scoredDf$`POS/NEG` <- as.numeric(scoredDf$`POS/NEG`) 
s <-head(scoredDf %>% arrange(`POS/NEG`) %>% select(text),10) 
#mais negativas segundo o naive bayes
s[,1]

#Mais positivas segundo Naive Bayes
s<-head(scoredDf %>% arrange(desc(`POS/NEG`)) %>% select(text),10) 

s[,1]

library(SentimentAnalysis)
dictionaryPortuguese <- SentimentDictionaryBinary(positivas$word, 
                                              negativas$word)
twdf$id <- row.names(twdf)
sentiment <- analyzeSentiment(twdf$tweet,
                              language="portuguese",
                              rules=list("PtSentiment"=list(ruleSentiment, dictionaryPortuguese), 
                                         "Ratio"=list(ruleSentimentPolarity,dictionaryPortuguese),
                                         "Words"=list(ruleWordCount)))
#sentiment
plotSentiment(sentiment)

#Pacote SentimentAnalysis para análise do sentimento dos conteúdos textuais em R
library(SentimentAnalysis)
dictionaryPortuguese <- SentimentDictionaryBinary(positivas$word, 
                                              negativas$word)
twdf$id <- row.names(twdf)
sentiment <- analyzeSentiment(twdf$tweet,
                              language="portuguese",
                              rules=list("PtSentiment"=list(ruleSentiment, dictionaryPortuguese), 
                                         "Ratio"=list(ruleSentimentPolarity,dictionaryPortuguese),
                                         "Words"=list(ruleWordCount)))
#sentiment
plotSentiment(sentiment)
sentiment$id <- row.names(sentiment)
worstfeelings <- sentiment[order(sentiment$PtSentiment),][1:10,] 
bestfeelings <-  sentiment[order(-sentiment$PtSentiment),][1:10,] 

##Topdown 10 Ruins
twdf[twdf$id %in% worstfeelings$id,]

#top 10
twdf[twdf$id %in% bestfeelings$id,]

#Bonus: Machine Learning com RTextTools
## Usando diferentes algoritimos para  testar Classificando
library(RTextTools)
library(ptstem)

classificados <- NULL
classificados <- read_csv('d://Cursos/PreparacaoCarreiraCientista/R-Bigdata/Projeto1/parte2/twitterclassificado.csv')
#pegando somente os que eu classifiquei manualmente, afinal..
classificados <- classificados[1:70,]
classificados$class.text[classificados$class.text== -1] <- 0
classificados$whois <- NULL
#renomeando colunas para converter em corpus, assim evitando a perda de acentuação
classificados$doc_id <- row.names(classificados)
classificados <- rename(classificados, text = tweet)

classificados$text <- sapply(classificados$text, function(x) ptstem(x, algorithm = "hunspell", complete = T))

classificados$text <- sapply(classificados$text,enc2native)
corp <- VCorpus(DataframeSource(classificados), readerControl = list(language='pt'))
corp <- tm_map(corp, stripWhitespace)

doc_matrix <- DocumentTermMatrix(corp)
set.seed(1234)
container <- create_container(doc_matrix, classificados$class.text, trainSize=1:55,
                              testSize=56:70, virgin=FALSE)
#removendo nnet
allalgos <- print_algorithms()

algos <- allalgos[! allalgos == 'NNET']
models <- train_models(container, algorithms=algos)

results <- classify_models(container, models)

# mostrando resultados
analytics <- create_analytics(container, results)
summary(analytics)

#Poderiamos tambêm fazer um cross_validate para testar qual algoritimo usar:
getAccuracy <- function(container, nfold, algoritmos){
  c <- 0
  d <- data.frame(algoritmo=as.numeric(0), meanAccuracy= as.numeric(0), deviation=as.numeric(0))
  for(i in algoritmos){
      print(paste('processando: ', i))
      ma <-  cross_validate(container,nfold,algorithm =i)
      if(c == 0) {
          d$algoritmo <- i
          d$meanAccuracy <- ma$meanAccuracy
          d$deviation <- sd(unlist(ma[[1]]))
          c <- 1
      }else {
        d<-rbind(d, data.frame(algoritmo=i, meanAccuracy = ma$meanAccuracy, deviation=sd(unlist(ma[[1]]))))
      }
  }
  return(d)
}
d <- getAccuracy(container,10, algos)  

# que temos erros no glmnet..longer object length is not a multiple of shorter object length
d
